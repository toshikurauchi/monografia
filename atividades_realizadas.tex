\section{Atividades realizadas}
\label{atividades}
As ideias apresentadas a seguir foram aplicadas no desenvolvimento da engine e da aplicação de visualização de campos tensoriais.

\subsection{Objetos da API gráfica}

Nessa seção serão descritos os mapeamentos utilizados pela engine para os objetos mais comuns definidos pelo OpenGL. 
Todos os mapeamentos foram projetados de forma que os objetos pudessem ser utilizados fora de um contexto inicializado e, 
além disso, os o objeto do OpenGL só é instanciado se necessário, o que economiza recursos da placa gráfica.

Como a implementação dos objetos dessa seção é altamente dependente da API
gráfica, eles devem ser construidos por um componente chamado
\texttt{GraphicObjectsFactory}. Esse componente pode ser obtido a partir de uma
instância de GraphicAPI da seguinte forma:

\begin{codigo}
GraphicAPI * gfx = ...;
GraphicObjectsFactory * factory = gfx->getFactory();
\end{codigo}

\vsection {Buffer}

Um buffer é uma região de memória controlado pelo driver gráfico. O conceito de
Buffer foi criado para resolver o gargalo da comunicação entre Memória Principal e Memória de Vídeo.

No OpenGL, buffers são criados através da função \texttt{glGenBuffers}. Depois
de criado, o buffer deve ser associado ao contexto do OpenGL, para isso
utiliza-se o comando \texttt{glBindBuffer}, esse comando, além de informar ao
OpenGL que o buffer deve ser associado ao contexto, faz com que os dados presentes no buffer sejam interpretados de forma diferente. Por exemplo, para informar que o buffer será utilizado para fornecer vértices ao pipeline, utiliza-se:

\begin{codigo}
glBindBuffer(GL_ARRAY_BUFFER);
\end{codigo}

Com o buffer associado ao contexto, é necessário inicializar seus dados.
Essa inicialização é feita pelo comando \texttt{glBufferData}. Após esses procedimentos, o buffer está pronto para ser utilizado.

% Colocar \texttt nos GL_WRITE_ONLY e outras constantes
Uma das características mais importantes do \texttt{buffer} é a possibilidade de mapeá-lo em uma região de memória que pode ser acessada pela aplicação. Esse mapeamento é importante, pois ele permite que a aplicação informe ao OpenGL que política de acesso a aplicação adotará para ler ou escrever na memória mapeada. Por exemplo, se o buffer for mapeado utilizando-se GL\_WRITE\_ONLY, o driver gráfico não precisa copiar os dados da memória do driver para a memória principal, o que diminui a troca de dados entre a memória de vídeo e a memória principal.

As políticas de acesso definidas pelo OpenGL são: 
\begin{itemize}
\item \texttt{GL\_READ\_ONLY}: quando a aplicação planeja apenas ler a região de
memória
\item \texttt{GL\_WRITE\_ONLY}: quando a região de memória for utilizada apenas
para escrita
\item \texttt{GL\_READ\_WRITE}: quando a memória mapeada for utilizada para
leitura e escrita
\end{itemize}

O mapeamento de um vertex buffer que será utilizado somente para escrita pode ser feito através do seguinte código:

\begin{codigo}
glBindBuffer(GL_ARRAY_BUFFER, bufferID);
void * mappedBuffer = glMapBuffer(GL_ARRAY_BUFFER, GL_WRITE_ONLY);
\end{codigo}

Após a utilização da região mapeada, é importante cancelar o mapeamento feito. Esse cancelamento é feito através da função \texttt{glUnmapBuffer};

% Precisa desses texttt?
Na Pandora's Box, buffers são implementações da interface
\texttt{Buffer}. O buffer padrão do OpenGL é implementado pela classe GLBuffer. 
A seguir é apresentado um exemplo de uso:

\begin{codigo}
GraphicAPI *gfx = ...;

// cria um buffer de tamanho size e com política de uso 'usage'
// As políticas de uso permitidas são:
// STREAM_DRAW, STREAM_READ, STREAM_COPY, STATIC_DRAW, STATIC_READ, 
// STATIC_COPY, DYNAMIC_DRAW, DYNAMIC_READ e DYNAMIC_COPY
Buffer *buffer = gfx->getFactory()->createBuffer(size, usage);
void * mapped = buffer->map(pbge::Buffer::WRITE_ONLY);

// operar na região mapeada
buffer->unmap();
\end{codigo}

\vsection{Shaders}

Como explicado anteriormente, o OpenGL opera nas primitivas gráficas através 
de um pipeline, que tem cinco estágios principais: transformação de vértices, 
tesselagem, criação de novas primitivas, rasterização e operações por fragmento. 
Com exceção da rasterização, todos os estágios podem ser customizados pelo uso de shaders.

No OpenGL, shaders são criados através da função \texttt{glCreateShader}, que 
retorna um handler para o shader criado, com o handler devolvido, é possível 
especificar o código fonte que será utilizado utilizando-se a função \texttt{glShaderSource}. 
Após associar um código fonte ao shader, ele pode ser compilado por \texttt{glCompileShader}. 
Para utilizar o shader criado, ele deve ser associado a um programa.

Programas, no OpenGL, são criados por \texttt{glCreateProgram}, que retorna o 
handler para o programa criado. Depois de criado, shaders podem ser associado
ao programa, a associação é feita pela função \texttt{glAttachShader}. Quando
todos os shaders necessários estiverem associados ao programa, ele pode ser 
preparado para execução através do comando \texttt{glLinkProgram}. Para sobrescrever 
o pipeline com o programa criado utiliza-se a função \texttt{glUseProgram}.

Na Pandora's Box, shaders são implementações da classe \texttt{Shader}. Para que possam ser 
utilizados para sobrescrever o pipeline, os shaders devem ser associados a um GPUProgram. 
A criação de um GPUProgram que sobrescreve tanto a transformação de vértices quanto o 
processamento de pixels é exemplificada abaixo:

\begin{codigo}
GraphicAPI * gfx = ...;

Shader * vertexShader = gfx->createShaderFromString(
				vertexShaderSource,
				pbge::Shader::VERTEX_SHADER);
Shader * fragmentShader = gfx->createShaderFromString(
				fragmentShaderSource,
				pbge::Shader::FRAGMENT_SHADER);

std::vector<Shader*> vertexShaders;
std::vector<Shader*> fragmentShaders;
vertexShaders.push_back(vertexShader);
fragmentShaders.push_back(fragmentShader);

GPUProgram * program = gfx->getFactory()->createProgram(
				vertexShaders,
				fragmentShaders);
\end{codigo}

O código acima pode ainda ser simplificado para:

\begin{codigo}
GraphicAPI * gfx = ...;
GPUProgram * program = createProgramFromString(
			vertexShaderSource, fragmentShaderSource);
\end{codigo}

O \texttt{GPUProgram} criado é utilizado para sobrescrever o pipeline através do código:

\begin{codigo}
GraphicAPI * gfx = ...;
GPUProgram * program = ...;
gfx->getState()->useProgram(program);
\end{codigo}


\vsection{Texturas}

Texturas no OpenGL são simplesmente tabelas de valores. Em aplicações mais antigas, as 
texturas eram utilizadas quase exclusivamente para o armazenamento de imagens que seriam 
mapeadas nos objetos da cena ou máscaras (em técnicas avançadas), porém com o
surgimento da computação genérica em GPU (GPGPU), as texturas passaram a ser utilizadas 
de forma análoga aos vetores e matrizes de linguagens como C++.

No OpenGL as texturas são criadas com o comando \texttt{glGenTextures}, que gera um handler 
para o objeto vazio criado, em seguida para é necessário associar o objeto criado ao contexto 
do OpenGL, o que é feito pelo comando \texttt{glBindTexture}. Com a textura
vazia associada ao contexto, é possível inicializar seus dados através de um comando da família \texttt{glTexImage}.

Na Pandora's Box, texturas são implementações da interface \texttt{Texture}. A inicialização 
de uma textura 2D é demonstrada no código abaixo:

\begin{codigo}
GraphicAPI * gfx = ...;

// image é uma interface que representa uma fonte de dados
// para uma textura 2D.
Image * image = ...;
Texture2D * texture = gfx->getFactory()->create2DTexture();

// especifica a imagem e como os dados devem ser representados
// na GPU
texture->setImageData(image, pbge::Texture::RGBA);
\end{codigo}

Com o aumento da quantidade de dados enviados à GPU através de texturas, 
surge um problema, as texturas padrão das APIs gráficas conseguem indexar 
apenas um pequeno número de pixels na textura, cerca de 2048 em placas recentes. 
Para resolver esse problema, o OpenGL introduziu o conceito de texture buffer, 
uma textura de uma dimensão que utiliza um buffer para armazenar dados, 
essa nova textura consegue indexar no mínimo 65536 pixels, segundo a
especificação do OpenGL 4.1~\cite{opengl41}.

A criação de um buffer texture é feita de forma ligeiramente diferente de uma textura convencional. 
Após a criação do handler, a textura deve ser associada ao contexto através da chamada 
\texttt{glBindTexture(GL\_TEXTURE\_BUFFER, handler)}, após a associação, o buffer que será utilizado 
pela textura é definido pelo comando \texttt{glTexBuffer}.

Na engine, os texture buffer são texturas que implementam a interface \texttt{TextureBuffer}. 
Um exemplo de criação de texture buffer é dado no código abaixo:

\begin{codigo}
GraphicAPI *gfx = ...

TextureBuffer * texture = 
		gfx->getFactory()->createTextureBuffer(size);
				
// a textura será representada como um conjunto de 4 floats
// por texel (elemento da textura)
texture->setInternalFormat(pbge::Texture::FLOAT, 
			   pbge::Texture::RGBA);

// recupera o buffer associado à textura
Buffer *buffer = texture->getBuffer();
float * data = (float*) buffer->map(pbge::Buffer::WRITE_ONLY);

// inicialização dos dados do buffer
buffer->unmap();
\end{codigo}

Após a criação das texturas, elas são enviadas ao shader através dos 
\texttt{UniformSet}, como demonstrado abaixo no caso de uma textura 2D:

\begin{codigo}
GraphicAPI *gfx = ...;
Texture2D * texture = ...;
UniformSet uniforms;

// associa a textura texture à variável do shader do tipo sampler2D
// chamada shader_texture
uniforms->getSampler2D("shader_texture")->setValue(texture);
gfx->pushUniforms(&uniforms);
\end{codigo}




\vsection{VertexBuffer}
No início da computação gráfica, os vértices eram especificados um a um através de 
chamadas à API gráfica, por exemplo, a especificação de um triângulo no OpenGL era feita da seguinte maneira:

\begin{codigo}
glBegin(GL_TRIANGLES);
glVertex3f(0.0f, 0.0f, 0.0f);
glNormal3f(0.0f, 0.0f, 1.0f);
glVertex3f(0.0f, 1.0f, 0.0f);
glNormal3f(0.0f, 0.0f, 1.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glNormal3f(0.0f, 0.0f, 1.0f);
glEnd();
\end{codigo}

É possível observar que o número de chamadas de função cresce linearmente com a quantidade de vértices 
do modelo, além disso cada um dos atributos do vértice (posição, cor, normal, entre outros) era 
especificado através de uma chamada de função diferente.

\begin{wrapfigure}{r}{0.6\textwidth}
\vspace{-20pt}
\begin{center}
\includegraphics[width=0.6\textwidth]{vertexbuffer}
\end{center}
\vspace{-20pt}
\caption{O vertex buffer\label{vertexbuffer}}
\vspace{-10pt}
\end{wrapfigure}

Com o aumento da complexidade dos modelos, foi necessário reinventar a API existente, 
com isso foi criado o conceito de vertex array, que é simplesmente um vetor de 
valores de tipos variados em que cada atributo é especificado através de seu tipo primitivo 
(double, float, int, short, byte) número de componentes, significado (posição, vetor normal, 
coordenada de textura, etc) e a distância em bytes para o próximo valor desse
atributo no vetor. Essa estrutura é exemplificada na figura ao lado.

Como a especificação desse modelo de dados era flexível e permitia enviar uma grande quantidade 
de dados para a GPU em um número constante de chamadas de função, ele foi rapidamente 
adotado pelos programadores.

Mas esse modelo não solucionava o problema de tráfego de dados entre a memória principal (RAM) 
e a memória de vídeo (VRAM). A solução foi colocar o vertex array dentro de um buffer 
gerenciado pela implementação da API gráfica, assim, a implementação poderia colocar dados muito 
utilizados dentro de regiões de fácil acesso, como a VRAM. Essa solução ficou conhecida como Vertex Buffer

Na Pandora's Box, Vertex Buffers são especificados através da classe
\texttt{VertexBuffer}, sua estrutura interna é semelhante à apresentada na
Figura~\ref{vertexbuffer}.
Cada vértice é especificado através da classe \texttt{VertexAttrib} que guarda
informações sobre o primeiro índice do atributo dentro o buffer, seu significado, 
número de componentes e qual a distância entre valores consecutivos do atributo.

A forma recomendada de se criar um \texttt{VertexBuffer} é através do \texttt{VertexBufferBuilder}.

% como construir com o builder: texto explicando?
\begin{codigo}
pbge::VertexBuffer * criaVertexBuffer(pbge::GraphicAPI * gfx) {
	// Inicializa com o número de vértices desejado
    int nVertices = ...;
    
    pbge::VertexBufferBuilder builder(nVertices);
    pbge::VertexAttribBuilder vertex = 
		builder.addAttrib(4, VertexAttrib::VERTEX);
    pbge::VertexAttribBuilder color = 
		builder.addAttrib(4, VertexAttrib::COLOR);
    				
    for(int i = 0; i < nVertices; i++) {
        float x, y, z, w; // Inicializados com os valores desejados
        float r, g, b, a; // Inicializados com os valores desejados
        
        builder.on(vertex).pushValue(x, y, z, w);
        builder.on(color).pushValue(r, g, b, a);
    }
    return builder.done(Buffer::STATIC_DRAW, gfx);
}
\end{codigo}


\vsection {FrameBufferObject}

O framebuffer é o destino dos pixels gerados através do pipeline. Existem dois tipos de framebuffer:

\begin{itemize}
\item O framebuffer do sistema de janelas: é o framebuffer que deve ser utilizado quando os pixels devem ser enviados para a janela da aplicação.
\item framebuffer virtual: é o framebuffer utilizado para renderização direta em texturas.
\end{itemize}

Os framebuffer objects encapsulam o segundo tipo. No OpenGL, framebuffer objects são criados pela função \texttt{glGenFramebuffersEXT}, que gera um handler que representa o objeto criado. Para associar uma textura ao framebuffer object criado utiliza-se a função \texttt{glFramebufferTexture2DEXT} e, por fim, para adicionar um buffer de profundidade, para permitir testes de profundidade, usa-se \texttt{glFramebufferTexture2DEXT}.

Para utilizar o framebuffer object criado, é necessário associá-lo ao contexto do OpenGL com o comando \texttt{glBindFramebufferEXT} e em seguida redirecionar os pixels gerados pelo pipeline através do comando \texttt{glDrawBuffers}.

Na engine desenvolvida, framebuffer objects são implementações da interface \texttt{FramebufferObject} e devem ser instanciados através da \texttt{GraphicObjectsFactory}. A criação e uso de framebuffer objects dentro da Pandora's Box é exemplificado abaixo:

\begin{codigo}
GraphicAPI * gfx = ...;
Texture2D * color = ...;
Texture2D * depth = ...;
FramebufferObject * fbo = 
		gfx->getFactory()->createFramebuffer(width, height);

// associa os valores escritos pelo shader na variável color_out à
// textura color
fbo->addRenderable(color, "color_out");

// usa depth como buffer de profundidade
fbo->setDepthRenderable(depth);

// associa o framebuffer object ao contexto
gfx->bindFramebufferObject(fbo);
\end{codigo}



\subsection{Grafo de cena}
A Pandora's Box utiliza um grafo enraizado, direcionado, sem circuitos, conhecido como grafo de cena, para representar a estrutura de uma cena. As mudanças realizadas por cada nó são aplicadas somente a si mesmo e aos seus filhos.

Para realizar a renderização da cena são realizadas buscas em profundidade no grafo até que todas as informações necessárias tenham sido obtidas. Esse processo será melhor explicado em ~\ref{visitors}.

\subsubsection{Tipos de nós}

%TODO: Verificar se falta informação
A engine define quatro tipos de nós padrão: \texttt{TransformationNode}, \texttt{CameraNode}, \texttt{ModelInstance} e \texttt{ModelCollection} (transformação, câmera, modelo e coleção de modelos respectivamente). 

Para implementar nós com comportamentos customizados basta criar uma nova classe filha de Node ou de algum de seus descendentes. A classe Node define essencialmente os métodos de atualização (\texttt{updatePass} e \texttt{postUpdatePass}) e renderização (\texttt{renderPass} e \texttt{postRenderPass}) que devem ser implementados por seus filhos, além de outros métodos específicos da estrutura do grafo.

Os métodos de atualização são chamados para preparar a cena para a renderização (inicialização de variáveis ou atualização de valores, por exemplo).

\begin{itemize}
% Isso esta suficiente?
\item \textbf{TransformationNode}: Esse nó guarda uma matriz de transformação $T$. No \texttt{updatePass} e \texttt{renderPass} a matriz corrente $M$ é armazenada e multiplicada por $T$ e o resultado é utilizado como a nova matriz de transformação corrente. Então no \texttt{postUpdatePass} e \texttt{postRenderPass} $M$ é reatribuída à matriz corrente.
\item \textbf{CameraNode}: O nó possui uma instância da classe Camera que é responsável por receber os parâmetros de câmera (configurações de posição e campo de visão), calcular a matriz de transformação a partir dessas informações e atualizar o estado do OpenGL para utilizá-la. Todas essas ações são realizadas no \texttt{updatePass}.
\item \textbf{ModelInstance}: O método \texttt{renderPass} é responsável por adicionar os shaders e suas uniformes e então solicitar a renderização do modelo pela API gráfica. O método \texttt{postRenderPass} retira as uniformes adicionadas.
\item \textbf{ModelCollection}: A implementação desse nó é análoga à do \texttt{ModelInstance}, com a diferença de que a quantidade de instâncias a serem renderizadas é recebida no construtor e enviada na solicitação da renderização do modelo pela API gráfica.
\end{itemize}

\subsubsection{Node Visitors}
\label{visitors}

Como foi explicado, o grafo de cena é um grafo enraizado, 
direcionado e sem circuitos, um Node Visitor é uma classe 
que dado a raíz do grafo de cena, consegue percorrer os nós 
do grafo obedecendo as seguintes regras:

\begin{itemize}
\item Todos os caminhos do grafo devem ser percorridos, se possível.
\item O estado do visitor ao visitar um dado nó A, deve 
depender apenas das modificações feitas por nós dentro do caminho da 
raíz do grafo até A.
\end{itemize}

O segundo item da lista acima, faz com que o grafo de cena represente uma estrutura hierárquica.

Dentro da engine, existem duas implementações concretas do visitor descrito:
\texttt{UpdaterVisitor} e \texttt{ColorPassVisitor}.

O \texttt{UpdaterVisitor} é um visitor encarregado de passar por cada
nó do grafo de cena e chamar o método \texttt{updatePass}, visitar todos os nós 
filhos do nó atual, chamar o método \texttt{postUpdatePass} e por fim atualizar
o bounding box do nó atual, como exemplificado no código abaixo:

\begin{codigo}
void UpdaterVisitor::dfsVisit(Node * node, GraphicAPI * gfx) {
    node->updatePass(this, gfx);
    
    for(...) // Node * child in node->getChildren()
        dfsVisit(child, gfx);
        
    node->postUpdatePass(this, gfx);
    if(node->getBoundingVolume() != NULL) {
        node->getBoundingVolume()->update(
					getCurrentTransformation());
    }
}
\end{codigo}

O \texttt{ColorPassVisitor} é uma implementação concreta da classe abstrata 
\texttt{RenderVisitor} que tem a função de chamar os métodos \texttt{renderPass} 
e \texttt{postRenderPass} do nó durante a execução dos métodos \texttt{visitAction} 
e \texttt{postVisitAction}, respectivamente, do \texttt{RenderVisitor}:

\begin{codigo}
class ColorPassVisitor : public RenderVisitor {
public:
    void visitAction(Node * node, GraphicAPI * gfx) {
        node->renderPass(this, gfx);
    }
    void postVisitAction(Node * node, GraphicAPI * gfx) {
        node->postRenderPass(this, gfx);
    }
};
\end{codigo}

O \texttt{RenderVisitor} é a classe base de todos os \texttt{visitors} que fazem renderização. 
Para extendê-la, a classe filha deve implementar dois métodos: \texttt{visitAction} e 
\texttt{postVisitAction}. Esse visitor é importante, pois implementa uma técnica
chamada frustum culling.

Frustum culling é o processo de renderizar apenas objetos que são visíveis para
a câmera atual. Na Pandora's Box, um nó é considerado não visível se ele não 
colide com o frustum da câmera corrente e ao ser considerado não
visível, ele e seus nós filhos não são visitados pelo \texttt{RenderVisitor}. 
Esse teste de visibilidade é executado dentro do método \texttt{dfsVisit} do 
\texttt{RenderVisitor}:

\begin{codigo}
void RenderVisitor::dfsVisit(Node * node, GraphicAPI * gfx) {
    AABB * boundingVolume = node->getBoundingVolume();
    
    if(boundingVolume == NULL || 
       boundingVolume->frustumCullingTest(boundingFrustum)) {
        visitAction(node, gfx);
        
        for(...) // Node * child in node->getChildren()
            dfsVisit(child, gfx);
            
        postVisitAction(node, gfx);
    }
}
\end{codigo}

\begin{center}
\begin{longtable}{cc}
\epsfig{file=frustum1.eps, width=0.47\textwidth,clip=} &
\epsfig{file=frustum2.eps, width=0.47\textwidth,clip=}
\end{longtable}
\vspace{-10pt}
\parbox{0.98\textwidth}{\captionof{figure}{Com o frustum culling os objetos que
estão fora do campo de visão da câmera não são enviados para o pipeline.}}
\end{center}

\subsection{Renderizador}

O renderizador da engine gráfica utiliza um algoritmo de 3 fases:

\begin{itemize}
\item Atualização dos nós do grafo de cena
\item Processamento do grafo de cena
\item Pós-Processamento da imagem gerada pela fase de processamento
\end{itemize}

\vsection {Fase de atualização}

Nessa fase, um \texttt{UpdaterVisitor} é utilizado para visitar e atualizar todos 
os nós do grafo de cena. É a única fase não customizável do algoritmo do renderizador.

\vsection {Fase de processamento do grafo de cena}

Nessa fase executa-se uma sequência de algoritmos definida pelo usuário da engine, a execução é equivalente ao código:

%verificar se os argumentos estão corretos
\begin{codigo}
std::vector<SceneProcessor*>::iterator it;
for(it = processors.begin(); it != processors.end(); it++){
   if(it->isActive()){
       it->process(gfx, renderer);
   }
}
\end{codigo}

Cada algoritmo deve implementar a interface \texttt{SceneProcessor}, 
que tem os seguintes métodos:

\begin{itemize}
\item \texttt{bool isInitialized(GraphicAPI*)}: método que indica se o algoritmo já preparou todas as suas dependências
\item \texttt{void initialize(GraphicAPI*, Renderer*)}: esse método deve criar todas as dependencias do método \texttt{process}
\item \texttt{void process(GraphicAPI*, Renderer*)}: executa o algoritmo de processamento
\item \texttt{bool isActive()}: indica se o algoritmo deve ou não ser executado
\end{itemize}

\vsection{Fase de pós-processamento}

Essa fase é semelhante à anterior, porém o teste de profundidade do fragmento não é executado por padrão, 
a implementação do algoritmo deve ativá-lo manualmente.

A interface que deve ser implementada é a \texttt{ScenePostProcessor} que define métodos com a mesma 
semântica dos métodos do \texttt{SceneProcessor}.

\subsubsection{Algoritmos de processamento de cena pré-definidos}

A engine atualmente define apenas um algoritmo de processamento padrão, o \texttt{RenderPassProcessor}. 
Esse algoritmo utiliza \texttt{ColorPassVisitor} para renderizar os nós do grafo de cena.

\subsubsection{Algoritmos de pós-processamento de cena pré-definidos}
 
Atualmente existem 2 algoritmos de pós-processamento padrão implementados pelas
classes: \texttt{FramebufferImageProcessor} e \texttt{BlitToFramebuffer}.

O \texttt{FramebufferImageProcessor} é utilizado para fazer o pós-processamento 
da imagem que estiver no buffer chamado color dentro do \texttt{FramebufferObject} atual. 
O algoritmo executado por esse objeto é descrito pelo código abaixo:

\begin{codigo}
void FramebufferImageProcessor::process(GraphicAPI *gfx,
					Renderer *renderer) {
    std::map<std::string, Texture2D*> & renderables;
    renderables = renderer->getRenderables();
    
    Texture2D * auxBuffer = renderables["color_aux"];
    Texture2D * colorBuffer = renderables["color"];
    
    renderables["color"] = auxBuffer;
    renderables["color_aux"] = colorBuffer;
    
    FramebufferObject * fbo = renderer->getFramebufferObject();
    fbo->removeRenderable("color");
    fbo->addRenderable(auxBuffer, "color");
    fbo->update(gfx);
    
    UniformSampler2D* sampler = 
        dynamic_cast<UniformSampler2D*>(gfx->getUniformValue(
			UniformInfo("color", pbge::SAMPLER_2D)));
        				
    sampler->setValue(colorBuffer);
    renderer->renderScreenQuad(program.get());
}
\end{codigo}

O \texttt{BlitToFramebuffer} renderiza um retângulo com as dimensões 
da janela com a textura de nome \texttt{"color"} armazenada no renderizador. É utilizado 
para renderizar a imagem armazenada na textura color para o framebuffer
do sistema de janelas.

\subsubsection{Algoritmos de pós-processamento customizados}

Foram desenvolvidos alguns algoritmos de pós-processamento customizados como exemplo. 
Eles estão disponíveis na aplicação de visualização de campos tensoriais. A implementação 
de cada algoritmo se resume a um fragment shader utilizado para instanciar um 
\texttt{FramebufferImageProcessor}. Esse fragment shader recebe a posição do fragmento 
e uma textura contendo a imagem a ser renderizada.

\begin{itemize}
\item \textbf{Inversor de cores:} O algoritmo de inversão de cores cria um vetor com 
os componentes \texttt{r,g,b}, calcula seu complemento e utiliza o resultado como 
a cor do fragmento:

\begin{codigo}
pbge::FramebufferImageProcessor * colorInversor() {
    return new pbge::FramebufferImageProcessor(
        "uniform sampler2D color;\n"
        "varying vec2 position;\n"
        "void main() {\n"
        "   vec3 color = (texture2D(color, position.xy)).rgb;\n"
        "   color = 1 - color;\n"
        "   gl_FragColor = vec4(color, 1);\n"
        "}\n"
    );
}
\end{codigo}
\item \textbf{Filtro de vermelho:} O filtro lê o componente veremelho da cor 
enviada pelo vertex shader e a utiliza no fragmento, sendo todos os outros 
componentes iguais a zero:

\begin{codigo}
pbge::FramebufferImageProcessor * chooseRed() {
    return new pbge::FramebufferImageProcessor(
        "uniform sampler2D color;\n"
        "varying vec2 position;\n"
        "void main() {\n"
        "   float r = (texture2D(color, position.xy)).r;\n"
        "   gl_FragColor = vec4(r, 0, 0, 1);\n"
        "}\n"
    );
}
\end{codigo}
\item \textbf{Lente senoidal:} A posição $(x_0, y_0)$ recebida é tal que 
$0 \leq x_0, y_0 \leq 1$. Ela é então mapeada para $(x_1, y_1)$, onde 
$-1 \leq x_0, y_0 \leq 1$. É calculado então o seno das componentes $x,y$ 
da posição multiplicadas por um fator que aumenta o efeito da lente. 
O resultado pertence ao intervalo $[-1,1]$, entretanto as coordenadas de 
textura estão no intervalo $[0,1]$. Por esse motivo o seno é multiplicado 
por $0.5$ e somado a $0.5$ resultando em um valor no mesmo intervalo das 
coordenadas de textura. Esse valor é utilizado para ler uma posição com um 
pequeno deslocamento da posição do fragmento atual, com isso é gerada uma 
leve deformação que simula o efeito de uma lente:

\begin{codigo}
pbge::FramebufferImageProcessor * senoidalLens() {
    return new pbge::FramebufferImageProcessor(
        "varying vec2 position;\n"
        "uniform sampler2D color;\n"
        "void main(){\n"
        "   vec2 x = 2 * position - 1.0;\n"
        "   gl_FragColor = "
        "        texture2D(color, 0.5 + 0.5 * sin(1.5 * x));\n"
        "}"
    );
}
\end{codigo}
\end{itemize}

\subsection{Mecanismos da Engine}

\subsubsection{Mapeamento e gerenciamento dos estados}

Em placas de vídeo modernas, dispositivos altamente paralelos, as trocas de 
estado podem fazer com que o pipeline gráfico tenha que ser esvaziado, o 
que pode causar um grande impacto no desempenho da aplicação, por isso, é 
necessário evitar trocas de estado redundantes, minimizar trocas de estado 
e agrupar as trocas de estado.

Na Pandora's Box, esse trabalho é delegado à classe \texttt{StateSet} e à duas
classes que controlam estados que podem ser desabilitados,
\texttt{BlendController} e \texttt{DepthController}. 

O \texttt{StateSet} é uma classe que controla a associação de objetos ao
contexto gráfico. As modificações feitas através da \texttt{StateSet} são
sempre acumuladas e só ocorrem quando o método \texttt{updateState} é chamado.
O \texttt{updateState} primeiro atualiza as associações dos objetos gráficos ao
contexto e em seguida atualiza os parâmetros do shader.

O \texttt{BlendController} controla a combinação de fragmentos no framebuffer.
Após ser gerado, se o fragmento não for descartado pelo depth test, ele é
enviado ao framebuffer e substitui o pixel correspondente, porém utilizando o
\texttt{BlendController}, pode-se fazer com que o fragmento gerado se combine de
forma diferente com o pixel existente no framebuffer.

O \texttt{DepthController} configura o depth test, o teste de visibilidade que é
feito após o fragment shader. Com esse controller é possível, por exemplo
desabilitar o depth test.



\subsubsection{Desenho de modelos}

Existem 2 modos de se enviar vértices para serem processados pelo pipeline, 
através de \texttt{VertexBuffer} ou através de chamadas de função obsoletas 
definidas pelo OpenGL, além disso, algumas vezes deseja-se que um mesmo modelo seja 
renderizado diversas vezes em um loop (instanced draw). Para lidar com essas situações, 
a Pandora's Box utiliza a classe \texttt{DrawController}.

O \texttt{DrawController} é responsável por enviar corretamente ao pipeline 
modelos definidos por instâncias de \texttt{VertexBuffer} (na engine tais 
modelos são instâncias de \texttt{VBOModel}) assim como instâncias de modelos 
que utilizam as funções obsoletas. Além de gerenciar a renderização de um modelo, 
o \texttt{DrawController} é responsável por implementar o instanced draw.

Na Pandora's Box, instanced draw é implementado de forma nativa para \texttt{VBOModel}, 
ou seja, utilizando-se as funções específicas do OpenGL que implementam a técnica, e 
de forma simulada se o modelo não for instância de VBOModel. 
A versão simulada da técnica é conhecida como pseudo-instanciação. Em ambas as versões, 
uma variável que indica qual é a instância do modelo que está atualmente sendo renderizada. 
Essa técnica pode ser ilustrada pelo código abaixo:

\begin{codigo}
GraphicAPI * gfx = ...;
Model * model = ...;

// prepara o modelo para a renderização
model->beforeRender(gfx);

for(int i = 0; i < number_of_instances; i++) {
    int instanceID = i;
    
    // renderiza a instância instanceID
    // a variável instanceID fica disponível no vertex shader
    model->render(gfx)
}

model->afterRender(gfx);
instanceID = 0;
\end{codigo}

\subsubsection{Passagem de parâmetros para o GPUProgram}

A customização do pipeline através de shaders gera grande flexibilidade, 
porém essa customização só é interessante devido a possibilidade de passagem de diferentes parâmetros para o shader.

Um programa do OpenGL pode receber três tipos de parâmetros:

% falar sobre built-in
\begin{itemize}
\item Uniformes: um valor que é constante para uma dada primitiva, 
  por exemplo, para um triângulo, o processamento de seus três vértices utilizam
  o mesmo valor de uniforme.
\item Uniformes built-in: são valores que se comportam como uniformes mas que
  são enviados automaticamente pelo OpenGL. As matrizes de transformação fazem
  parte dessa classe de valores.
\item Atributos: um valor que é constante para um vértice. Atributos só podem ser 
  acessados dentro do \texttt{vertex shader}. No exemplo acima, cada um dos
vértices do triângulo poderia ter um valor diferente para o atributo.
\end{itemize}

Dentro da Pandora's Box, o mecanismo de passagem de parâmetros para o shader 
é implementado através das classes \texttt{UniformSet}, \texttt{UniformStack},
\texttt{GPUProgram}, \texttt{UniformValue} e \texttt{BuiltInUniformBinder} para
uniformes e da classe \texttt{AttribBinder} para atributos.

Como foi citado anteriormente, a última etapa da atualização de estados 
é a sincronização dos parâmetros do shader. Nessa fase, inicialmente, cada 
um dos \texttt{UniformInfo} gerados durante a compilação do shader é utilizado 
para buscar um \texttt{UniformValue} dentro da \texttt{UniformStack}, o uniform 
value encontrado é então associado ao programa através do mecanismo ilustrado 
no código abaixo para o caso do GPUProgram implementado para OpenGL:

\begin{codigo}
void GLProgram::updateUniforms(GraphicAPI * gfx) {
    std::vector<UniformBindAndInfo>::iterator it;
    for(it = uniforms.begin(); it != uniforms.end(); it++) {
        UniformValue * value = gfx->searchUniform(it->getInfo());
        if(it->shouldUpdate(value)) {
            it->update(value);
            // associa o valor da uniforme ao shader
            value->bindValueOn(this, it->getInfo(), gfx);
        }
    }
}
\end{codigo}

% Talvez não citar todas as matrizes. Colocar referencias.

Após essa etapa de associação de valores, as uniformes built-in são 
enviadas ao shader. O envio de built-ins é feito
através dos \texttt{BuiltInUniformBinder}, que são classes especializadas 
para cada um dos tipos de valor.

O mecanismo para o envio dos atributos é semelhante ao utilizada para enviar 
as uniformes \texttt{built-in}. Para cada um dos tipos de atributo definidos na
\texttt{enum} \texttt{pbge::VertexAttrib::Type} existe uma classe
(\texttt{AttrBinder}) especializada em associar o atributo ao shader.

\subsection{Visualização de campos tensoriais}

A aplicação de visualização de campos tensoriais é dividida em duas etapas:

\begin{itemize}
\item Compilação do formato Analyze~\cite{Analyze} para o formato \texttt{.ctf}
(Compiled Tensor Field)
\item Apresentação do campo contido no arquivo \texttt{.ctg}
\end{itemize}

\subsubsection{O formato Analyze}

O formato Analyze é um formato de armazenamento de informações de imagens de ressonância magnética. A informação é dividida em dois arquivos: um cabeçalho (extensão \texttt{.hdr}) com informações sobre o campo (dimensões, ordenação, identificação e histórico) e o arquivo de imagem (extensão \texttt{.img}) contendo somente os valores da imagem (organizados conforme a descrição do cabeçalho).

Para o desenvolvimento do leitor de arquivos Analyze foi feita a suposição de que os nomes dos arquivos (\texttt{.hdr} e \texttt{.img}) são iguais visando simplificar a utilização e implementação.

\subsubsection{O formato Compiled Tensor Field (.ctf)}

O formato Compiled Tensor Field (\texttt{.ctf}) foi desenvolvido para armazenamento de informações sobre o campo tensorial a ser mostrado na aplicação de visualização de campos tensoriais. É um formato binário que contém o número de elipsóides (representação visual do tensor) no arquivo seguido por um conjunto de matrizes de transformação linear. Cada matriz será aplicada a uma esfera para obter um elipsóide na posição correta no campo. Por esse motivo ela contém uma escala (proporcional aos autovalores do tensor aplicados nos eixos cartesianos), uma rotação (dos eixos cartesianos para os eixos definidos pelos autovetores do tensor) e uma translação (para posicionar o elipsóide corretamente no campo).

Na etapa de compilação a imagem de ressonância magnética é lida e os tensores
armazenados em um vetor. Nessa etapa todos os tensores nulos são ignorados. Um
tensor $A_{3
\times 3}$ é considerado nulo quando o módulo de todas as suas entradas é
maior do que um dado $\varepsilon > 0$. Ou seja, $A$ é nulo quando a seguinte expressão é válida
para $i, j \in \mathbb{Z}$, $1 \leq i, j \leq 3$:

% Ficou melhor explicado assim?
\vspace{10pt}
\hspace{10pt}
\begin{math}
|A_{i,j}| < \varepsilon
\end{math}
\vspace{10pt}

São então calculados os autovalores e autovetores de todos os tensores não nulos, geradas as matrizes de transformação linear que levam uma esfera centralizada na origem para um elipsóide na posição correta no campo e armazenadas em um vetor. 

Como tais matrizes são matrizes de transformação homogênea, sabemos que a última linha sempre será $(0,0,0,1)$. Assim é possível ocultar tais dados e enviar outras informações em seu lugar (é necessário substituir os valores dessa linha para realizar quaisquer operações com a matriz). Nessa linha são armazenados os valores das equações (\ref{af}), (\ref{linearCase}), (\ref{planarCase}) e (\ref{sphericalCase}) (definidas na página \pageref{af}) que serão utilizados como diferentes políticas de escolha de nível de transparência (alfa) e cor dos elipsóides.

As matrizes são então reorganizadas em blocos de proximidade para otimizar a utilização pela aplicação de visualização. O vetor de matrizes reordenado é finalmente escrito no arquivo, além das informações iniciais sobre o campo.

\subsubsection{Apresentação do campo compilado}

O arquivo \texttt{.ctf} é lido e cada bloco de matrizes é enviado como \texttt{uniform} para o \texttt{shader} que as aplica a esferas. A política de transparência (alfa) dos elipsóides é escolhida pelo usuário, sendo o valor de alfa algum dos quatro resultados das equações de anisotropia fracionada. A cor aplicada a cada elipsóide é calculada a partir do valor de alfa em uma rampa de cores também definida pelo usuário.

\begin{center}
\begin{longtable}{cc}
\epsfig{file=cerebro1.eps, width=0.47\textwidth,clip=} &
\epsfig{file=cerebro2.eps, width=0.47\textwidth,clip=}
\end{longtable}
\vspace{-15pt}
\parbox{0.98\textwidth}{\captionof{figure}{Imagens da
visualização do campo tensorial de um cérebro humano. As cores são aplicadas a partir do valor de
anisotropia fracionada em uma rampa de cores que varia do azul (menor
anisotropia) para o amarelo (maior anisotropia).}}
\end{center}

Para a translação e rotação do campo foram implementados um \texttt{KeyboardEventHandler} e um \texttt{MouseEventHandler} respectivamente que atualizam os nós de transformação.

\subsection{Técnicas aplicadas}

\subsubsection{Depth Peeling}
No campo tensorial em diversos casos existem elipsóides sobrepostos. Para observar alguns tipos de estrutura é interessante que tais elipsóides possuam algum nível de transparência. Para isso é necessário simular a transparência através de combinações de cores sobrepostas. Existem técnicas~\cite{alphasorting} que dependem dos objetos serem renderizados dos mais distantes para os mais próximos da câmera, entretanto no caso do campo esse tipo de processo se torna muito lento devido à grande quantidade de objetos a serem ordenados antes de cada renderização. Por esse motivo é necessário algum algoritmo independente da ordem de renderização.

Depth peeling~\cite{everitt} é uma técnica iterativa que consiste de remoções de camadas próximas a cada iteração. Inicialmente a cena é renderizada normalmente armazenando o color buffer e o depth buffer em buffers auxiliares. A cada nova iteração todos os fragmentos com profundidade menor ou igual à profundidade armazenada no depth buffer auxiliar são descartados. Um novo depth buffer auxiliar é gerado a partir das profundidades dos fragmentos restantes, que são então renderizados e o resultado (color buffer) é acumulado no color buffer auxiliar.

Para a realização da técnica completa são necessárias $N$ iterações, onde $N$ é o número máximo de fragmentos sobrepostos na cena. Entretanto foi fixado um número de iterações para diminuir a complexidade da técnica.

Na aplicação de exemplo o depth peeling foi implementado como um processador de cena sendo que a cada iteração o grafo de cena é percorrido uma vez.

As imagens abaixo exemplificam essa técnica com duas esferas acompanhadas do resultado da iteração.

\begin{wrapfigure}{l}{\textwidth}
\vspace{-20pt}
\begin{center}
\subfigure[Primeira iteração]{\includegraphics[width=0.49\textwidth]{depthpeeling1}}
\subfigure[Segunda iteração]{\includegraphics[width=0.49\textwidth]{depthpeeling2}}
\subfigure[Terceira iteração]{\includegraphics[width=0.49\textwidth]{depthpeeling3}}
\end{center}
\vspace{-20pt}
\caption{Em cada iteração do depth peeling é desconsiderada uma camada de
pixels. A imagem é combinada ao resultado anterior considerando-se a
opacidade de cada fragmento.}
\vspace{-10pt}
\end{wrapfigure}
